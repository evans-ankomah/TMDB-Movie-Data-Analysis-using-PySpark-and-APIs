{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#  TMDB Movie Analysis Pipeline\n",
                "\n",
                "This notebook orchestrates a complete ETL pipeline for movie data analysis using PySpark and the TMDb API.\n",
                "\n",
                "## Pipeline Steps:\n",
                "1. **Extract** - Fetch movie data from TMDb API\n",
                "2. **Transform** - Clean and prepare data using PySpark  \n",
                "3. **Analyze** - Run KPI analysis and rankings\n",
                "4. **Visualize** - Generate charts and insights"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup & Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "\n",
                "# Add project root to path\n",
                "sys.path.insert(0, os.getcwd())\n",
                "\n",
                "from pyspark.sql import SparkSession\n",
                "from src.extract import fetch_all_movies\n",
                "from src.transform import clean_and_transform, show_data_summary\n",
                "from src.analyze import run_all_analysis, display_analysis_results\n",
                "from src.visualize import save_all_visualizations\n",
                "from utils.config import OUTPUT_DIR\n",
                "from utils.logger import setup_logger\n",
                "\n",
                "# Initialize logger\n",
                "logger = setup_logger()\n",
                "\n",
                "print(\" Imports successful!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Initialize Spark Session"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create Spark session\n",
                "spark = SparkSession.builder \\\n",
                "    .appName(\"TMDB Movie Analysis\") \\\n",
                "    .config(\"spark.driver.memory\", \"4g\") \\\n",
                "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
                "    .getOrCreate()\n",
                "\n",
                "spark.sparkContext.setLogLevel(\"WARN\")\n",
                "print(\"Spark session created!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Step 1: Extract Data from TMDb API\n",
                "\n",
                "Fetch movie details using the TMDb API with retry mechanism and rate limiting."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Extract movie data from API\n",
                "raw_movies = fetch_all_movies()\n",
                "\n",
                "print(f\"\\n Extracted {len(raw_movies)} movies from TMDb API\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Step 2: Transform & Clean Data\n",
                "\n",
                "Clean the raw data:\n",
                "- Extract values from JSON columns\n",
                "- Convert data types\n",
                "- Handle missing values\n",
                "- Filter valid movies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Clean and transform the data\n",
                "df = clean_and_transform(spark, raw_movies)\n",
                "\n",
                "# Display summary\n",
                "show_data_summary(df)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Step 3: Analyze Data & Generate KPIs\n",
                "\n",
                "Run comprehensive analysis including:\n",
                "- Movie rankings (revenue, budget, profit, ROI, ratings)\n",
                "- Advanced search queries\n",
                "- Franchise vs Standalone comparison\n",
                "- Director performance analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run all analysis\n",
                "analysis_results = run_all_analysis(df)\n",
                "\n",
                "# Display results\n",
                "display_analysis_results(analysis_results)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Step 4: Create Visualizations\n",
                "\n",
                "Generate and save visualizations:\n",
                "- Revenue vs Budget trends\n",
                "- ROI by Genre\n",
                "- Popularity vs Rating\n",
                "- Yearly box office trends\n",
                "- Franchise vs Standalone comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate visualizations\n",
                "viz_files = save_all_visualizations(df, analysis_results)\n",
                "\n",
                "print(\"\\n Visualizations saved:\")\n",
                "for name, path in viz_files.items():\n",
                "    print(f\"    {name}: {path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Display Visualizations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from IPython.display import Image, display\n",
                "\n",
                "# Display saved visualizations\n",
                "for name, path in viz_files.items():\n",
                "    if os.path.exists(path):\n",
                "        print(f\"\\n {name.replace('_', ' ').title()}:\")\n",
                "        display(Image(filename=path, width=800))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Pipeline Summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"    PIPELINE COMPLETED SUCCESSFULLY!\")\n",
                "print(\"=\"*60)\n",
                "print(f\"\\n Results saved to: {OUTPUT_DIR}/\")\n",
                "print(f\" Visualizations created: {len(viz_files)}\")\n",
                "print(f\" Movies analyzed: {df.count()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Cleanup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Stop Spark session when done\n",
                "spark.stop()\n",
                "print(\" Spark session stopped.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.14.2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
